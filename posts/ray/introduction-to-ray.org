#+BEGIN_COMMENT
.. title: Introduction to Ray
.. slug: introduction-to-ray
.. date: 2021-11-11 18:30:38 UTC-08:00
.. tags: ray,tutorial
.. category: Ray
.. link: 
.. description: The first ray tutorial.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 3
#+PROPERTY: header-args :session ~/.local/share/jupyter/runtime/kernel-422ee6a1-8f3d-47f8-a01f-ab386f96ffae-ssh.json

#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
# %load_ext autotime
#+END_SRC
* A Simple Data Parallel Example
This comes from the [[https://github.com/ray-project/tutorial][ray-project tutorial]] on github. It is the first exercise they have and it shows you how to run functions in parallel on a single machine with multiple CPUs free. This makes it sort of the equivalent of what I already use [[https://joblib.readthedocs.io/en/latest/][joblib]] for, but it's a start.

** Imports
   We're going to make a function that sleeps as an example of a slow function that can benefit from running in parallel, so we'll use python's [[https://docs.python.org/3/library/time.html][time]] module both to sleep and time how long it takes to run. The only other requirement is /ray/, but I'm also using [[https://expects.readthedocs.io/en/stable/][expects]], just because I find it a little more readable than plain old ~assert~.
#+begin_src python :results none
# python standard library
import time

# from pypy
from expects import (be_above, be_below,
                     contain_exactly, expect)
import ray
#+end_src
* Starting Ray
 You start ray using [[https://docs.ray.io/en/latest/package-ref.html?highlight=init#ray.init][~ray.init~]]. In this case we're going to tell it the exact number of CPUs to use but normally you wouldn't pass this in and Ray will figure it out. We're also going to pass in ~ignore_reinit_error=True~ which tells it to ignore errors if we run the cell more than once.

#+begin_src python :results none
PARALLEL_CALLS = CPUS = 5
ray.init(num_cpus=CPUS, ignore_reinit_error=True)
#+end_src

* Function to Remote Function

We're going to define a function that just sleeps before returning whatever gets passed into it. Then, to make it something that can run in parallel, we'll add the [[https://docs.ray.io/en/latest/package-ref.html?highlight=ray.remote#ray.remote][~@ray.remote~]] decorator. Although it's called ~remote~ in this case it's going to run in multiple CPUs on the machine where it gets called. The decorator adds a method called called ~remote~ to your function which will take as arguments anything your function takes as arguments and pass it to the function when calling it.

The ~remote~ method (~slow_function.remote()~ not ~ray.remote~, kind of confusing these names) returns an object that you can use to get your functions output (using [[https://docs.ray.io/en/latest/package-ref.html?highlight=ray.get#ray-get][~ray.get~]]).

#+begin_src python :results none
SECONDS_TO_SLEEP = 1

@ray.remote
def sleepy(argument):
    time.sleep(SECONDS_TO_SLEEP)
    return argument
#+end_src

* So Run It Already

First we'll sleep a little to improve the accuracy of the timing measurements. We do this because workers may still be starting up in the background (probably not, in this case, since I'm typing this post, but in other cases maybe). We're telling it to sleep for one second, but there's also some overhead (a little from python and a little more from ray) so we're going to check that the amount added is within reason (1 second plus our tolerance).

#+begin_src python :results output :exports both
WAIT_FOR_WORKERS = 2
TOLERANCE = 0.1
EXPECTED_UPPER_BOUND_FOR_RUNTIME = SECONDS_TO_SLEEP + TOLERANCE

time.sleep(WAIT_FOR_WORKERS)
start_time = time.time()

results = ray.get([sleepy.remote(identifier) for identifier in range(PARALLEL_CALLS)])

end_time = time.time()
duration = end_time - start_time

print(f"This is what the function-calls returned: {results}.")
print(f"The non-parallel run time would have taken at least {PARALLEL_CALLS} seconds.")
print(f"The parallel version took {duration:0.2f} seconds.")

expect(results).to(contain_exactly(*list(range(PARALLEL_CALLS))))
expect(duration).to(be_below(EXPECTED_UPPER_BOUND_FOR_RUNTIME))
expect(duration).to(be_above(SECONDS_TO_SLEEP))
#+end_src

#+RESULTS:
: This is what the function-calls returned: [0, 1, 2, 3, 4].
: The non-parallel run time would have taken at least 5 seconds.
: The parallel version took 1.01 seconds.

So, there you go. By running the functions in parallel we got them to run in the about the amount it would have taken just one of them to run.

**Note:** I changed the number of sleepers from four to five and re-ran the init, but this didn't seem to update the number of CPUs ray was running so I used [[https://docs.ray.io/en/latest/package-ref.html?highlight=ray.shutdown#ray-shutdown][~ray.shutdown~]] to tear it down before re-running everything. I don't know if there's a more elegant way to do this (and in this case it's an artificial problem) but, just something to note. Maybe it wasn't a good idea to tell ~ray.init~ to ignore errors when re-initializing, because then I would have known you shouldn't do it.
