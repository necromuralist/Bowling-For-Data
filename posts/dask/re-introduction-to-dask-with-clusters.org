#+BEGIN_COMMENT
.. title: Re-Introduction to Dask: With Clusters
.. slug: re-introduction-to-dask-with-clusters
.. date: 2021-11-16 13:51:44 UTC-08:00
.. tags: dask,clusters,tutorial
.. category: Dask
.. link: 
.. description: Trying a distributed cluster with dask.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 3
#+PROPERTY: header-args :session ~/.local/share/jupyter/runtime/kernel-1995fb53-372f-4bbc-8f65-abdd230f03eb.json

#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
# %load_ext autotime
#+END_SRC
* A Simple Parallel Example (Take Two)
  This is a second stab at using [[https://distributed.dask.org/en/stable/index.html][dask distributed]]. I'm going to re-do the {{% lancelot "previous dask post" %}}introduction-to-dask{{% /lancelot %}} that I did but this time I'm going to see if I can get it working with a remote machine. 
** The Setup
   I used [[https://github.com/pyenv/pyenv][pyenv]] to get python 3.10 working (barely, dask doesn't seem to work well with [[https://fishshell.com/][fish]] - sometimes I have to fix the ~PATH~ variable by hand). On the same machine that I'm editing this notebook (and running the jupyter kernel for it) I ran the scheduler at the command line.

#+begin_src bash
dask-scheduler
#+end_src

Which gave me this output.

#+begin_src bash
distributed.scheduler - INFO - -----------------------------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - -----------------------------------------------
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:      tcp://10.26.0.6:8786
distributed.scheduler - INFO -   dashboard at:                     :8787
#+end_src

The address given doesn't match my laptop, though... another thing I'll have to look into.

Then on my remote server I ran the worker.

#+begin_src bash
dask-worker 192.168.86.137:8786
#+end_src

Which gave me this.

#+begin_src bash
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.86.165:33255'
distributed.worker - INFO -       Start worker at: tcp://192.168.86.165:37763
distributed.worker - INFO -          Listening to: tcp://192.168.86.165:37763
distributed.worker - INFO -          dashboard at:       192.168.86.165:39589
distributed.worker - INFO - Waiting to connect to:  tcp://192.168.86.137:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          8
distributed.worker - INFO -                Memory:                  31.28 GiB
distributed.worker - INFO -       Local Directory: /home/hades/projects/Bowling-For-Data/dask-worker-space/worker-uz55oaz3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:  tcp://192.168.86.137:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
#+end_src

I shouldn't have started it in that directory, but, whatever. It looks like it worked.
** Imports
   Once again, just the basic stuff.

#+begin_src python :results none
# python standard library
import time

# from pypi
from expects import (be_above, be_below,
                     contain_exactly, expect)
from dask.distributed import Client
#+end_src
* Starting Dask
  So, here's where it's a little different. Since the machine running the jupyter kernel is also the scheduler, when I start the client, I'll give it the localhost address and the port that the scheduler is using to connect to it. The [[https://distributed.dask.org/en/stable/worker.html][workers]] are now remote and the choice of the number of threads and processes to use is made when they are started up. I didn't pass in any arguments so it should only be one process with many threads.

#+begin_src python :results none
PARALLEL_CALLS = PROCESSES = 5
ADDRESS, PORT = "127.0.0.1", "8786"
client = Client(f"{ADDRESS}:{PORT}")
#+end_src

Let's see what workers have registered.

#+begin_src python :results output :exports both
print(client.ncores())
print(client.nthreads())
#+end_src

#+RESULTS:
: {'tcp://192.168.86.165:37763': 8}
: {'tcp://192.168.86.165:37763': 8}

So we have one worker with 8... threads I assume, not cores. That needs investigating too. Onward.

* Sleepy Function
  So here's the function that sleeps.

#+begin_src python :results none
SECONDS_TO_SLEEP = 1

def sleepy(argument):
    time.sleep(SECONDS_TO_SLEEP)
    return argument
#+end_src

* So Run It Already

  This code is exactly the same as the previous one. Let's see if it works.

#+begin_src python :results output :exports both
TOLERANCE = 0.1
EXPECTED_UPPER_BOUND_FOR_RUNTIME = SECONDS_TO_SLEEP + TOLERANCE * 4
PARALLEL_CALLS = 5

start_time = time.time()

futures = client.map(sleepy, range(PARALLEL_CALLS))
results = client.gather(futures)
end_time = time.time()
duration = end_time - start_time

print(f"This is what the function-calls returned: {results}.")
print(f"The non-parallel run time would have taken at least {PARALLEL_CALLS} seconds.")
print(f"The parallel version took {duration:0.2f} seconds.")

expect(results).to(contain_exactly(*list(range(PARALLEL_CALLS))))
expect(duration).to(be_below(EXPECTED_UPPER_BOUND_FOR_RUNTIME))
expect(duration).to(be_above(SECONDS_TO_SLEEP))
#+end_src

#+RESULTS:
: This is what the function-calls returned: [0, 1, 2, 3, 4].
: The non-parallel run time would have taken at least 5 seconds.
: The parallel version took 1.09 seconds.

So it takes about as long as running it on one machine did. In the previous post where I was running it on one machine I was using the same machine as the current worker so this makes sense.
** Check The Caching
   Let's make sure that the caching works as expected.

#+begin_src python :results output :exports both
start_time = time.time()

futures = client.map(sleepy, range(PARALLEL_CALLS))
results = client.gather(futures)
end_time = time.time()
duration = end_time - start_time

print(f"This is what the function-calls returned: {results}.")
print(f"The non-parallel run time would have taken at least {PARALLEL_CALLS} seconds.")
print(f"The parallel version took {duration:0.2f} seconds.")

expect(results).to(contain_exactly(*list(range(PARALLEL_CALLS))))
expect(duration).to(be_below(EXPECTED_UPPER_BOUND_FOR_RUNTIME))
# expect(duration).to(be_above(SECONDS_TO_SLEEP))
#+end_src

#+RESULTS:
: This is what the function-calls returned: [0, 1, 2, 3, 4].
: The non-parallel run time would have taken at least 15 seconds.
: The parallel version took 0.03 seconds.

Seems to work.

* Another Worker
  After a long ordeal (something has happened to Ubuntu's python I think - I upgraded my server and now trying to install "distributed" with ~pip~ causes it to crash with a ~AttributeError: module importlib._bootstrap has no attribute "SourceFileLoader"~ message, which supposedly is a pip mismatch, and even after wiping my server and starting with a fresh install it still happens - I had to download the dask distributed source (~git clone https://github.com/dask/distributed.git~) and install it from that to get it to work - between ~ray~ and ~dask~ this has been pretty hellacious trying to get a distributed cluster working). Anyway.

I started the new worker with the command:

#+begin_src bash
dask-worker "192.168.86.137:8786" --nprocs auto
#+end_src

This produced a much longer output than the previous ~dask-worker~ invocation so I won't show it. The ~--nprocs auto~ argument tells ~dask~ to figure out how many processes and threads to allocate on its own.

#+begin_src python :results output :exports both
print(client.ncores())
#+end_src

#+RESULTS:
: {'tcp://192.168.86.148:33745': 2, 'tcp://192.168.86.148:37901': 2, 'tcp://192.168.86.148:41715': 2, 'tcp://192.168.86.165:37763': 8}

So, I'm assuming that this means that the new worker has three processes with two threads each... maybe.

Let's try it again with more function calls than the 8 threads on the original worker. First I'll have to restart the client to get rid of the caching, though.

#+begin_src python :results output :exports both
client.restart()
TOLERANCE = 0.1
EXPECTED_UPPER_BOUND_FOR_RUNTIME = SECONDS_TO_SLEEP + TOLERANCE * 4
PARALLEL_CALLS = 15

start_time = time.time()

futures = client.map(sleepy, range(PARALLEL_CALLS))
results = client.gather(futures)
end_time = time.time()
duration = end_time - start_time

print(f"This is what the function-calls returned: {results}.")
print(f"The non-parallel run time would have taken at least {PARALLEL_CALLS} seconds.")
print(f"The parallel version took {duration:0.2f} seconds.")

expect(results).to(contain_exactly(*list(range(PARALLEL_CALLS))))
# expect(duration).to(be_below(EXPECTED_UPPER_BOUND_FOR_RUNTIME))
expect(duration).to(be_above(SECONDS_TO_SLEEP))
#+end_src

#+RESULTS:
: This is what the function-calls returned: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14].
: The non-parallel run time would have taken at least 15 seconds.
: The parallel version took 2.08 seconds.

So it still executes faster, but not as fast as it did... perhaps fiddling with the parameters would fix this, but that would be besides the point for such silly functions.

It seems to still work- the hard part is that it ran really fast without the cluster - so how do you know the extra machine even ran? I don't know yet.

** Check The Caching
   Let's make sure that the caching still works as expected.

#+begin_src python :results output :exports both
start_time = time.time()

futures = client.map(sleepy, range(PARALLEL_CALLS))
results = client.gather(futures)
end_time = time.time()
duration = end_time - start_time

print(f"This is what the function-calls returned: {results}.")
print(f"The non-parallel run time would have taken at least {PARALLEL_CALLS} seconds.")
print(f"The parallel version took {duration:0.2f} seconds.")

expect(results).to(contain_exactly(*list(range(PARALLEL_CALLS))))
expect(duration).to(be_below(EXPECTED_UPPER_BOUND_FOR_RUNTIME))
# expect(duration).to(be_above(SECONDS_TO_SLEEP))
#+end_src

#+RESULTS:
: This is what the function-calls returned: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14].
: The non-parallel run time would have taken at least 15 seconds.
: The parallel version took 0.02 seconds.

Seems so.
