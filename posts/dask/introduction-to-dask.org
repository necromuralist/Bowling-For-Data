#+BEGIN_COMMENT
.. title: Introduction to Dask
.. slug: introduction-to-dask
.. date: 2021-11-15 18:30:38 UTC-08:00
.. tags: dask,tutorial
.. category: Dask
.. link:
.. description: The dask interpretation of the first ray tutorial.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 3
#+PROPERTY: header-args :session ~/.local/share/jupyter/runtime/kernel-96f97957-284d-46df-a0a4-53ce0a063240-ssh.json

#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
# %load_ext autotime
#+END_SRC
* A Simple Parallel Example
  This is a first stab at using [[https://distributed.dask.org/en/stable/index.html][dask distributed]]. I'm going to re-do the {{% lancelot "first ray post" %}}introduction-to-ray{{% /lancelot %}} that I did using dask instead. I tried dask a while ago to see if I could use it in place of python to speed things up, but at the time I was doing a lot of string cleaning and manipulation and it didn't seem to handle it very well. Anyway, this time I'm going to see if I can use it to make a mini cluster. The two advantages it might have over ray are that it works with pypy and I can't get ray's clustering to work. I guess the second point is the bigger one for now. I might revisit ray later and see if I can figure out why the nodes won't connect.
  Dask seems to be closer to ray in the way it works than it is to joblib. Joblib is simpler to use but it's more of a one-and done thing from what I can see, while ray and dask use the "futures" idea so you can have a little more flexibility - we'll see, I haven't even gotten it to work yet.

**Note:** I couldn't get dask to work with pypy. Every time I tried to start the client it crashed the interpreter (after being stuck for a very long time). Hopefully this is something that I can work around at some point, but after the ray debacle I'm just trying to get /something/ working so this is being run using python 3.10 (using [[https://github.com/pyenv/pyenv][pyenv]] to pick the python versions. Getting ~pyenv~ to work was another debacle but at least I found another temporary workaround to get things running).

** Imports
   We're going to make a function that sleeps as an example of a slow function that can benefit from running in parallel, so we'll use python's [[https://docs.python.org/3/library/time.html][time]] module both to sleep and time how long it takes to run. The only other requirement is /dask/, but I'm also using [[https://expects.readthedocs.io/en/stable/][expects]], just because I find it a little more readable than plain old ~assert~.
#+begin_src python :results none
# python standard library
import time

# from pypi
from expects import (be_above, be_below,
                     contain_exactly, expect)
from dask.distributed import Client
#+end_src
* Starting Dask
  You start dask by creating a [[https://distributed.dask.org/en/stable/client.html][Client]] object. In this case I'm only going to run it on the local machine. When you do this it creates a [[https://distributed.dask.org/en/stable/api.html#distributed.LocalCluster][LocalCluster]] which takes an ~n_workers~ argument, which I believe is the number of CPUs we can use. It's not exactly that straight-forward in that it's going to create "Workers" and we can then tell it how many ~threads_per_worker~ to use, as well as whether to use processes or threads (the default being processes). So do we try to match it using multiple workers or number of threads? I guess the match is the number of threads. I'll try it and see, anyway.

 According to [[https://stackoverflow.com/questions/49406987/how-do-we-choose-nthreads-and-nprocs-per-worker-in-dask-distributed][this stackoverflow post]], the default is to creates a single process and as many threads as the machine has cores. The poster (MRocklin) says that the choice of workers or threads depends on what you're doing. The threads are python threads and so are bound by the Global Interpreter Lock, but if you are doing a lot of numeric computing which is thus using numpy or other stuff not bound by the lock, then using fewer processes and more threads is better because it reduces the inter-process communication, while if you're manipulating python objects that are then bound by the lock, then you're better off using more processes. But the more your workers need to communicate with each other, more overhead the extra processes will create. 
  Good to know, 

#+begin_src python :results none
PARALLEL_CALLS = PROCESSES = 5
client = Client(threads_per_worker=PROCESSES)
#+end_src

* Function to Remote Function


#+begin_src python :results none
SECONDS_TO_SLEEP = 1

def sleepy(argument):
    time.sleep(SECONDS_TO_SLEEP)
    return argument
#+end_src

* So Run It Already

First we'll sleep a little to improve the accuracy of the timing measurements. We do this because workers may still be starting up in the background (probably not, in this case, since I'm typing this post, but in other cases maybe). We're telling it to sleep for one second, but there's also some overhead (a little from python and a little more from ray) so we're going to check that the amount added is within reason (1 second plus our tolerance).

#+begin_src python :results output :exports both
TOLERANCE = 0.1
EXPECTED_UPPER_BOUND_FOR_RUNTIME = SECONDS_TO_SLEEP + TOLERANCE

start_time = time.time()

futures = client.map(sleepy, range(PARALLEL_CALLS))
results = client.gather(futures)
end_time = time.time()
duration = end_time - start_time

print(f"This is what the function-calls returned: {results}.")
print(f"The non-parallel run time would have taken at least {PARALLEL_CALLS} seconds.")
print(f"The parallel version took {duration:0.2f} seconds.")

expect(results).to(contain_exactly(*list(range(PARALLEL_CALLS))))
expect(duration).to(be_below(EXPECTED_UPPER_BOUND_FOR_RUNTIME))
expect(duration).to(be_above(SECONDS_TO_SLEEP))
#+end_src

#+RESULTS:
: This is what the function-calls returned: [0, 1, 2, 3, 4].
: The non-parallel run time would have taken at least 5 seconds.
: The parallel version took 1.04 seconds.

So it does about as well as ray did. One interesting aspect of dask, though, is that it expects to be given [[https://en.wikipedia.org/wiki/Pure_function]["pure" functions]], meaning no side-effects and a particular input will always give the same output. Because of that second condition, it will (by default, it can be overridden) cache the outputs for a certain input, so calling the same function with the same inputs won't actually execute the function over again. I'll run the exact same code again and we can see how this affects the running time.

#+begin_src python :results output :exports both
start_time = time.time()

futures = client.map(sleepy, range(PARALLEL_CALLS))
results = client.gather(futures)
end_time = time.time()
duration = end_time - start_time

print(f"This is what the function-calls returned: {results}.")
print(f"The non-parallel run time would have taken at least {PARALLEL_CALLS} seconds.")
print(f"The parallel version took {duration:0.2f} seconds.")

expect(results).to(contain_exactly(*list(range(PARALLEL_CALLS))))
expect(duration).to(be_below(EXPECTED_UPPER_BOUND_FOR_RUNTIME))
# expect(duration).to(be_above(SECONDS_TO_SLEEP))
#+end_src

#+RESULTS:
: This is what the function-calls returned: [0, 1, 2, 3, 4].
: The non-parallel run time would have taken at least 5 seconds.
: The parallel version took 0.03 seconds.

So now we can see that the overhead takes about 0.03 seconds (although where that occurred is hard to say) and that no matter how long our functions might have taken the first time, they would always take some very small amount the second time they're called with the same arguments.
